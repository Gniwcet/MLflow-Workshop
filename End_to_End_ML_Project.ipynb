{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment <br>\n",
    "\n",
    "Python 3.8 <br>\n",
    "Numpy: 1.23.0 <br>\n",
    "Pandas: 1.5.3 <br>\n",
    "matplotlib: 3.7.1 <br>\n",
    "seaborn: 0.10.1 <br>\n",
    "Scikit-Learn: 1.1.3 <br>\n",
    "MLFlow: 1.30.0 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib  #\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn  #\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, RocCurveDisplay\n",
    "from sklearn.model_selection import KFold\n",
    "import mlflow\n",
    "import mlflow.sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy: 2.3.2\n",
      "Pandas: 2.3.2\n",
      "matplotlib: 3.10.6\n",
      "seaborn: 0.13.2\n",
      "Scikit-Learn: 1.7.1\n",
      "MLFlow: 3.3.2\n"
     ]
    }
   ],
   "source": [
    "print(\"Numpy: {}\".format(np.__version__))\n",
    "print(\"Pandas: {}\".format(pd.__version__))\n",
    "print(\"matplotlib: {}\".format(matplotlib.__version__))\n",
    "print(\"seaborn: {}\".format(sns.__version__))\n",
    "print(\"Scikit-Learn: {}\".format(sklearn.__version__))\n",
    "print(\"MLFlow: {}\".format(mlflow.__version__))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the dataset from this link : https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'creditcard.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m data_path = \u001b[33m\"\u001b[39m\u001b[33mcreditcard.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m df = df.drop(\u001b[33m\"\u001b[39m\u001b[33mTime\u001b[39m\u001b[33m\"\u001b[39m, axis=\u001b[32m1\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# remove time column since it was found that it is not helpful\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Ecole/Campus DevOps/MLflow-Workshop/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Ecole/Campus DevOps/MLflow-Workshop/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Ecole/Campus DevOps/MLflow-Workshop/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Ecole/Campus DevOps/MLflow-Workshop/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Ecole/Campus DevOps/MLflow-Workshop/.venv/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'creditcard.csv'"
     ]
    }
   ],
   "source": [
    "data_path = \"creditcard.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "df = df.drop(\"Time\", axis=1)\n",
    "# remove time column since it was found that it is not helpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data to normal and anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m normal = \u001b[43mdf\u001b[49m[df.Class == \u001b[32m0\u001b[39m].sample(frac=\u001b[32m0.5\u001b[39m, random_state=\u001b[32m2020\u001b[39m).reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m) \n\u001b[32m      2\u001b[39m anomaly = df[df.Class == \u001b[32m1\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "normal = df[df.Class == 0].sample(frac=0.5, random_state=2020).reset_index(drop=True) \n",
    "anomaly = df[df.Class == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Normal: {normal.shape}\")\n",
    "print(f\"Anomaly: {anomaly.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into train/ dev/ test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_train, normal_test = train_test_split(normal, test_size = 0.2, random_state = 2020)\n",
    "anomaly_train, anomaly_test = train_test_split(anomaly, test_size = 0.2, random_state = 2020)\n",
    "normal_train, normal_validate = train_test_split(normal_train,test_size = 0.25, random_state = 2020)\n",
    "anomaly_train, anomaly_validate = train_test_split(anomaly_train, test_size = 0.25, random_state = 2020)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate set to creates X-Y sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.concat((normal_train, anomaly_train))\n",
    "x_test = pd.concat((normal_test, anomaly_test))\n",
    "x_validate = pd.concat((normal_validate,anomaly_validate))\n",
    "\n",
    "y_train = np.array(x_train[\"Class\"])\n",
    "y_test = np.array(x_test[\"Class\"])\n",
    "y_validate = np.array(x_validate[\"Class\"])\n",
    "\n",
    "x_train = x_train.drop(\"Class\", axis=1)\n",
    "x_test = x_test.drop(\"Class\", axis=1)\n",
    "x_validate = x_validate.drop(\"Class\", axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training sets:\\nx_train: {} \\ny_train:{}\".format(x_train.shape, y_train.shape))\n",
    "print(\"\\nTesting sets:\\nx_test: {} \\ny_test:{}\".format(x_test.shape, y_test.shape))\n",
    "print(\"\\nValidation sets:\\nx_validate: {} \\ny_validate: {}\".format(x_validate.shape, y_validate.shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(pd.concat((normal, anomaly)).drop(\"Class\", axis=1))\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "x_validate = scaler.transform(x_validate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(sk_model, x_train, y_train): \n",
    "    sk_model = sk_model.fit(x_train, y_train)\n",
    "    train_acc = sk_model.score(x_train, y_train)\n",
    "    mlflow.log_metric(\"train_acc\", train_acc)\n",
    "    print(f\"Train Accuracy: {train_acc:.3%}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sk_model, x_test, y_test):\n",
    "    eval_acc = sk_model.score(x_test, y_test)\n",
    "    preds = sk_model.predict(x_test)\n",
    "    auc_score = roc_auc_score(y_test, preds)\n",
    "    # ask MLFlow to log two more metrics\n",
    "    mlflow.log_metric(\"eval_acc\", eval_acc)\n",
    "    mlflow.log_metric(\"auc_score\", auc_score)\n",
    "    print(f\"Auc Score: {auc_score:.3%}\")\n",
    "    print(f\"Eval Accuracy: {eval_acc:.3%}\")\n",
    "    roc_plot = plot_roc_curve(sk_model, x_test, y_test,name='Scikit-learn ROC Curve')\n",
    "    plt.savefig(\"sklearn_roc_plot.png\")\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_test, preds)\n",
    " \n",
    "    ax = sns.heatmap(conf_matrix, annot=True,fmt='g') \n",
    "    ax.invert_xaxis()\n",
    "    ax.invert_yaxis()\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.title(\"Confusion Matrix\") \n",
    "    plt.savefig(\"sklearn_conf_matrix.png\")\n",
    "    # save the plots generated by matplotlib and by seaborn.\n",
    "    mlflow.log_artifact(\"sklearn_roc_plot.png\")\n",
    "    mlflow.log_artifact(\"sklearn_conf_matrix.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log and View MLFlow Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_model = LogisticRegression(random_state=None, max_iter=400, solver='newton-cg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# puts the run under experiment name\n",
    "mlflow.set_experiment(\"Credit_Card_Fraud_experiment\") \n",
    "mlflow.set_tracking_uri(\"localhost:5000\")\n",
    "# chunk all code under the context of one MLFlow run.\n",
    "with mlflow.start_run():\n",
    "    train(sk_model, x_train, y_train)\n",
    "    evaluate(sk_model, x_test, y_test)\n",
    "    # save the model\n",
    "    mlflow.sklearn.log_model(sk_model, \"log_reg_model\")\n",
    "    print(\"Model run: \", mlflow.active_run().info.run_uuid)\n",
    "mlflow.end_run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On terminal, write: mlflow ui -p 1234\n",
    "so you can access mlflow UI\n",
    "\n",
    "and then go to 127.0.0.1:1234"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a Logged Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = mlflow.sklearn.load_model(\"runs:/d96759e62aaf44dba44dc50575de258a/log_reg_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.score(x_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLFlow Parameter Tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broad Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_weights = [1, 5, 10, 15]\n",
    "num_folds = 5\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True, random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"sklearn_creditcard_broad_search\") \n",
    "logs = []\n",
    "for f in range(len(anomaly_weights)):\n",
    "    fold = 1\n",
    "    accuracies = []\n",
    "    auc_scores= []\n",
    "    for train, test in kfold.split(x_validate, y_validate):\n",
    "        with mlflow.start_run():\n",
    "            weight = anomaly_weights[f] \n",
    "            mlflow.log_param(\"anomaly_weight\", weight)\n",
    "            class_weights= {0: 1,1: weight }\n",
    "            sk_model = LogisticRegression(random_state=None, max_iter=400,\n",
    "                                        solver='newton-cg',\n",
    "                                        class_weight=class_weights).fit(x_validate[train],y_validate[train])\n",
    "            for h in range(40): print('-', end=\"\") \n",
    "            print(f\"\\nfold {fold}\\nAnomaly Weight: {weight}\")\n",
    "            train_acc = sk_model.score(x_validate[train], y_validate[train])\n",
    "            mlflow.log_metric(\"train_acc\", train_acc)\n",
    "            eval_acc = sk_model.score(x_validate[test], y_validate[test])\n",
    "            preds = sk_model.predict(x_validate[test])\n",
    "            mlflow.log_metric(\"eval_acc\", eval_acc)\n",
    "\n",
    "            try:\n",
    "                auc_score = roc_auc_score(y_validate[test], preds)\n",
    "            except:\n",
    "                auc_score = -1\n",
    "            mlflow.log_metric(\"auc_score\", auc_score)\n",
    "            print(\"AUC: {}\\neval_acc: {}\".format(auc_score,\n",
    "            eval_acc))\n",
    "            accuracies.append(eval_acc)\n",
    "            auc_scores.append(auc_score)\n",
    "            log = [sk_model, x_validate[test],\n",
    "            y_validate[test], preds]\n",
    "            logs.append(log)\n",
    "            mlflow.sklearn.log_model(sk_model,\n",
    "            f\"anom_weight_{weight}_fold_{fold}\")\n",
    "            fold = fold + 1\n",
    "            mlflow.end_run()\n",
    "    print(\"\\nAverages: \")\n",
    "    print(\"Accuracy: \", np.mean(accuracies))\n",
    "    print(\"AUC: \", np.mean(auc_scores))\n",
    "    print(\"Best: \")\n",
    "    print(\"Accuracy: \", np.max(accuracies))\n",
    "    print(\"AUC: \", np.max(auc_scores))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guided Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_weights = [10, 50, 150, 200]\n",
    "num_folds = 5\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True, random_state=2020)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new experiment for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"sklearn_creditcard_Guided_search\") \n",
    "logs = []\n",
    "for f in range(len(anomaly_weights)):\n",
    "    fold = 1\n",
    "    accuracies = []\n",
    "    auc_scores= []\n",
    "    for train, test in kfold.split(x_validate, y_validate):\n",
    "        with mlflow.start_run():\n",
    "            weight = anomaly_weights[f] \n",
    "            mlflow.log_param(\"anomaly_weight\", weight)\n",
    "            class_weights= {0: 1,1: weight }\n",
    "            sk_model = LogisticRegression(random_state=None, max_iter=400,\n",
    "                                        solver='newton-cg',\n",
    "                                        class_weight=class_weights).fit(x_validate[train],y_validate[train])\n",
    "            for h in range(40): print('-', end=\"\") \n",
    "            print(f\"\\nfold {fold}\\nAnomaly Weight: {weight}\")\n",
    "            train_acc = sk_model.score(x_validate[train], y_validate[train])\n",
    "            mlflow.log_metric(\"train_acc\", train_acc)\n",
    "            eval_acc = sk_model.score(x_validate[test], y_validate[test])\n",
    "            preds = sk_model.predict(x_validate[test])\n",
    "            mlflow.log_metric(\"eval_acc\", eval_acc)\n",
    "\n",
    "            try:\n",
    "                auc_score = roc_auc_score(y_validate[test], preds)\n",
    "            except:\n",
    "                auc_score = -1\n",
    "            mlflow.log_metric(\"auc_score\", auc_score)\n",
    "            print(\"AUC: {}\\neval_acc: {}\".format(auc_score,\n",
    "            eval_acc))\n",
    "            accuracies.append(eval_acc)\n",
    "            auc_scores.append(auc_score)\n",
    "            log = [sk_model, x_validate[test],\n",
    "            y_validate[test], preds]\n",
    "            logs.append(log)\n",
    "            mlflow.sklearn.log_model(sk_model,\n",
    "            f\"anom_weight_{weight}_fold_{fold}\")\n",
    "            fold = fold + 1\n",
    "            mlflow.end_run()\n",
    "    print(\"\\nAverages: \")\n",
    "    print(\"Accuracy: \", np.mean(accuracies))\n",
    "    print(\"AUC: \", np.mean(auc_scores))\n",
    "    print(\"Best: \")\n",
    "    print(\"Accuracy: \", np.max(accuracies))\n",
    "    print(\"AUC: \", np.max(auc_scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6df5fb1b25f0df95286e840a09a7d4433b68f80dbec77cb3e9268a8c02167994"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
